# -Data-flow-from-Kafka-to-Hadoop
This project aimed to build a data pipeline from Apache Kafka to Hadoop, enabling efficient collection, storage, and processing of large datasets. Data from various sources is collected through Kafka and then pushed to Hadoop for real-time storage and analysis.

Methods: Two approaches were proposed to implement the data pipeline:
- MongoDB -> Kafka -> Hadoop
- API -> Kafka -> Hadoop

Data Used: The data was sourced from the Aviationstack website, which provides real-time and historical flight information. The raw data was then processed and filtered to retain essential details such as flight codes, flight dates, departure and arrival airports, time zones, takeoff times, and airline names.

Results: The data pipeline was successfully built, and the data processing steps were carried out. The outcomes include data transmission flows from MongoDB and the API to Kafka, followed by data transfer to Hadoop. Reports and presentation slides outlining the project execution and results were also completed.
